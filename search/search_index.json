{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"(N)Grams","text":"<p>Simply create (N)grams: N ~ Bi | Tri ...</p> <p> </p> <p>Welcome to bigrams, a Python project that provides a non-intrusive way to connect tokenized sentences in (N)grams. This tool is designed to work with tokenized sentences, and it is focused on a single task: providing an efficient way to merge tokens from a list of tokenized sentences.</p> <p>It's non-intrusive as it leaves tokenisation, stopwords removal and other text preprocessing out of its flow.</p> <p>Source Code: https://github.com/proteusiq/bigrams</p> <p>PyPI: https://pypi.org/project/bigrams/</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install -U bigrams\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>To use bigrams, import it into your Python script, and use <code>scikit-learn</code>-ish API to transform your tokens.</p> <pre><code>from bigrams import Grams\n\n# expects tokenised sentences\nin_sentences = [[\"this\", \"is\", \"new\", \"york\", \"baby\", \"again!\"],\n              [\"new\", \"york\", \"and\", \"baby\", \"again!\"],\n            ]\ng = Grams(window_size=2, threshold=2)\n\nout_sentences = g.fit_transform(in_sentences)\nprint(out_sentences)\n# [[\"this\", \"is\", \"new_york\", \"baby_again!\"],\n#   [\"new_york\", \"and\", \"baby_again!\"],\n#  ]\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.7+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatters (e.g. <code>black</code>, <code>isort</code>), linters (e.g. <code>mypy</code>, <code>flake8</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"#contributing-are-welcome","title":"Contributing are welcome","text":""},{"location":"#todo","title":"ToDo:","text":"<ul> <li>[ ] Create Scikit-learn compatible transformer</li> <li>[ ] ~~create a save &amp; load function~~</li> <li>[ ] compare it with gensim Phrases</li> <li>[ ] write replacer in Rust - PyO3</li> </ul>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#replacer","title":"Replacer","text":"<p>Helper function, returns a list of tokens with (N)grams connected</p> <p>Parameters:</p> Name Type Description Default <code>sentence</code> <p>sentence in form of tokens with grams</p> required <code>bigrams</code> <p>a mapper of (t1, t2) =&gt; t1_t2</p> required <code>window_size</code> <p>how many tokens to be considers: default 2</p> <code>2</code> <p>Returns:</p> Name Type Description <code>sentence</code> <code>SentenceType</code> <p>sentence in form of tokens with (N)grams</p> <p>Usage:</p> <pre><code>from bigrams import replacer\n\nbigrams = {(\"new\", \"york\")}\nin_sentence = [\"this\", \"is\", \"new\", \"york\", \"baby\", \"again!\"]\nout_sentence = replacer(sentence=in_sentence,\n                        bigrams=bigrams,\n                        window_size=2,\n                )\nassert out_sentence == [\"this\", \"is\", \"new_york\", \"baby\", \"again!\"]\n</code></pre>"},{"location":"api_docs/#grams","title":"Grams","text":"<p>Grams allows  you to transform a list of tokens into a list of (N)grams tokens Arguments:     threshold: how many times should tokens appears together to be connected as ngrams     window_size: the N in (N)gram. how many words should be considered. defaults = 2 Usage:</p> <pre><code>from bigrams import Grams\nin_sentences = [[\"this\", \"is\", \"new\", \"york\", \"baby\", \"again!\"],\n             [\"new\", \"york\", \"and\", \"baby\", \"again!\"],\n            ]\ng = Grams(window_size=2, threshold=2)\nout_sentences = g.fit_transform(in_stences)\nout_sentences\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""}]}